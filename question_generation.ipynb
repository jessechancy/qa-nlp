{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Generation\n",
    "\n",
    "Purpose: Given an article output a list of sentences\n",
    "1. Parse Article into sentences\n",
    "2. From each sentence, generate Stanford dependency parse tree\n",
    "3. From each parse tree, use rule based method to generate question from sentence.\n",
    "4. Refine the sentences using language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article -> Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "for i in range(1, 10):\n",
    "    with open(f'./Development_data/set1/a{i}.txt', 'r') as f:\n",
    "        content.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for file in content:\n",
    "    sentences.extend(nltk.sent_tokenize(file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences -> Parse Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.corenlp import CoreNLPServer\n",
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "# nltk.download('wordnet')\n",
    "import os\n",
    "import requests\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "CoreNLPServerError",
     "evalue": "Could not connect to the server.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCoreNLPServerError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b82f7b1811e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m    \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTANFORD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stanford-corenlp-3.9.2-models.jar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/assignment_2/lib/python3.7/site-packages/nltk/parse/corenlp.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, stdout, stderr)\u001b[0m\n\u001b[1;32m    153\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCoreNLPServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Could not connect to the server.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCoreNLPServerError\u001b[0m: Could not connect to the server."
     ]
    }
   ],
   "source": [
    "STANFORD = os.path.join(\"models\", \"stanford-corenlp-full-2018-10-05\")\n",
    "\n",
    "# Create the server\n",
    "server = CoreNLPServer(\n",
    "   os.path.join(STANFORD, \"stanford-corenlp-3.9.2.jar\"),\n",
    "   os.path.join(STANFORD, \"stanford-corenlp-3.9.2-models.jar\"),    \n",
    ")\n",
    "server.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\n      \"tokens\": [\\n        {\\n          \"index\": 1,\\n          \"word\": \"data\",\\n          \"originalText\": \"data\",\\n          \"characterOffsetBegin\": 0,\\n          \"characterOffsetEnd\": 4,\\n          \"pos\": \"NN\",\\n          \"before\": \"\",\\n          \"after\": \"\"\\n        },\\n        {\\n          \"index\": 2,\\n          \"word\": \"=\",\\n          \"originalText\": \"=\",\\n          \"characterOffsetBegin\": 4,\\n          \"characterOffsetEnd\": 5,\\n          \"pos\": \"JJ\",\\n          \"before\": \"\",\\n          \"after\": \"\"\\n        },\\n        {\\n          \"index\": 3,\\n          \"word\": \"tmp\",\\n          \"originalText\": \"tmp\",\\n          \"characterOffsetBegin\": 5,\\n          \"characterOffsetEnd\": 8,\\n          \"pos\": \"NN\",\\n          \"before\": \"\",\\n          \"after\": \"\"\\n        }\\n      ]\\n    }\\n  ]\\n}\\n'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.post('http://[::]:9000/?properties={\"annotators\":\"tokenize,ssplit,pos\",\"outputFormat\":\"json\"}', data = {'data': \"tmp\"}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Stanford Parser: https://nlp.stanford.edu/software/lex-parser.shtml#Download Version 3.9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tree import Tree\n",
    "parser = CoreNLPParser()\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "sp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "invertible_aux_verb = {'am', 'are', 'is', 'was', 'were', 'can', 'could', 'does', 'did', 'has', 'had', 'have', 'may', 'might',\n",
    "                       'must', 'shall', 'should', 'will', 'would'}\n",
    "\n",
    "purge_tree = {\"PRN\"}\n",
    "\n",
    "#Do/Did for I only\n",
    "#Does/Did for everything else\n",
    "\n",
    "def is_invertible(s):\n",
    "    if isinstance(s, str):\n",
    "        return s.lower() in invertible_aux_verb\n",
    "    return False\n",
    "\n",
    "def list_to_string(word_list):\n",
    "    return ' '.join(word_list)\n",
    "\n",
    "def tree_to_string(parsed_tree, lower = False):\n",
    "#     if isinstance(parsed_tree, str):\n",
    "#         return parsed_tree\n",
    "#     words = []\n",
    "#     for subtree in parsed_tree:\n",
    "#         words.append(tree_to_string(subtree))\n",
    "    leaves = parsed_tree.leaves()\n",
    "    if lower:\n",
    "        leaves[0] = leaves[0].lower()\n",
    "    return list_to_string(leaves)\n",
    "\n",
    "def first(parsed_tree):\n",
    "    if isinstance(parsed_tree[0], str):\n",
    "        return parsed_tree\n",
    "    return first(parsed_tree[0])\n",
    "\n",
    "#purges tree based on the set purge_trees\n",
    "def purge(parsed_tree):\n",
    "    if isinstance(parsed_tree, str):\n",
    "        return False\n",
    "    length = len(parsed_tree)\n",
    "    i = 0\n",
    "    if parsed_tree.label() in purge_tree:\n",
    "        return True\n",
    "    while i < length:\n",
    "        res = purge(parsed_tree[i])\n",
    "        if res:\n",
    "            del parsed_tree[i]\n",
    "            length -= 1\n",
    "        else:\n",
    "            i += 1\n",
    "    return False\n",
    "\n",
    "def binary_question_from_tree(parsed_tree):\n",
    "    sentence = parsed_tree[0]\n",
    "    assert(sentence.label() == 'S')\n",
    "    np = sentence[0]\n",
    "    vp = sentence[1]\n",
    "    noun_label = first(np).label()\n",
    "    #print(\"NL\", noun_label)\n",
    "    assert(np.label() == 'NP')\n",
    "    assert(vp.label() == 'VP')\n",
    "    \n",
    "    #print(parsed_tree)\n",
    "    if noun_label in [\"NNP\", \"NNPS\"] or first(np)[0] in [\"The\", \"A\"]:\n",
    "        subject = tree_to_string(np, True) if noun_label == \"DT\" else tree_to_string(np)\n",
    "        #print(parsed_tree)\n",
    "        purge(vp)\n",
    "        remain = vp.leaves()[1:]\n",
    "        if is_invertible(first(vp)[0]): #checks if is aux word\n",
    "            return list_to_string([first(vp)[0].capitalize(), subject] + remain) + '?'\n",
    "        else:\n",
    "            #Add Does/Did/Do\n",
    "            verb_label = first(vp).label()\n",
    "            if isinstance(first(vp)[0], str):\n",
    "                lemmas = sp(first(vp)[0])\n",
    "                lemma = lemmas[0].lemma_\n",
    "            if verb_label in [\"VBP\", \"VBZ\",\"VBG\"]: #present tense\n",
    "                return list_to_string([\"Does\", subject, lemma] + remain) + \"?\"\n",
    "            elif verb_label in [\"VBD\", \"VBN\"]: #past tense\n",
    "                return list_to_string([\"Did\", subject, lemma] + remain) + \"?\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence Structure Tree\n",
    "class SST():\n",
    "    def __init__(self, label, children):\n",
    "        self.label = label\n",
    "        self.children = children\n",
    "\n",
    "#Sentence Structure Leaf\n",
    "class SSL():\n",
    "    def __init__(self, label):\n",
    "        self.label = label\n",
    "        \n",
    "simple_predicate = SST('ROOT', [SST('S', [SSL('NP'), SSL('VP'), SSL('.')])])\n",
    "\n",
    "def satisfies_structure(parsed_tree, structure):\n",
    "    if isinstance(structure, SSL):\n",
    "        return parsed_tree.label() == structure.label\n",
    "    else:\n",
    "        if parsed_tree.label() != structure.label or len(parsed_tree) != len(structure.children): return False\n",
    "        for i in range(len(parsed_tree)):\n",
    "            if satisfies_structure(parsed_tree[i], structure.children[i]) == False:\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences = [\"The company bought food for the homeless.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================== Sentence ======================\n",
      "Sentence: Egypt attained its first continuous peak of civilization – the first of three so-called \"Kingdom\" periods (followed by the Middle Kingdom and New Kingdom) which mark the high points of civilization in the lower Nile Valley.\n",
      "Question: Did Egypt attain its first continuous peak of civilization -- the first of three so-called `` Kingdom '' periods which mark the high points of civilization in the lower Nile Valley?\n",
      "=========================== Sentence ======================\n",
      "Sentence: The term itself was coined by eighteenth-century historians and the distinction between the Old Kingdom and the Early Dynastic Period is not one which would have been recognized by Ancient Egyptians.\n",
      "Question: Is the term itself was coined by eighteenth-century historians and the distinction between the Old Kingdom and the Early Dynastic Period not one which would have been recognized by Ancient Egyptians?\n",
      "=========================== Sentence ======================\n",
      "Sentence: The basic justification for a separation between the two periods is the revolutionary change in architecture accompanied by the effects on Egyptian society and economy of large-scale building projects.\n",
      "Question: Is the basic justification for a separation between the two periods the revolutionary change in architecture accompanied by the effects on Egyptian society and economy of large-scale building projects?\n",
      "=========================== Sentence ======================\n",
      "Sentence: The Old Kingdom is most commonly regarded as the period from the Third Dynasty through to the Sixth Dynasty (2686–2181 BC).\n",
      "Question: Is the Old Kingdom most commonly regarded as the period from the Third Dynasty through to the Sixth Dynasty?\n",
      "=========================== Sentence ======================\n",
      "Sentence: A new era of building was initiated at Saqqara under his reign.\n",
      "Question: Was a new era of building initiated at Saqqara under his reign?\n",
      "=========================== Sentence ======================\n",
      "Sentence: The Old Kingdom is perhaps best known for the large number of pyramids constructed at this time as burial places for Egypt's kings.\n",
      "Question: Is the Old Kingdom perhaps best known for the large number of pyramids constructed at this time as burial places for Egypt 's kings?\n",
      "=========================== Sentence ======================\n",
      "Sentence: The former rulers were forced to assume the role of governors or otherwise work in tax collection.\n",
      "Question: Were the former rulers forced to assume the role of governors or otherwise work in tax collection?\n",
      "=========================== Sentence ======================\n",
      "Sentence: Egyptians in this era worshipped their Pharaoh as a god, believing that he ensured the annual flooding of the Nile that was necessary for their crops.\n",
      "Question: Did Egyptians in this era worship their Pharaoh as a god , believing that he ensured the annual flooding of the Nile that was necessary for their crops?\n",
      "=========================== Sentence ======================\n",
      "Sentence: The first is called the Meidum pyramid, named for its location in Egypt.\n",
      "Question: Is the first called the Meidum pyramid , named for its location in Egypt?\n",
      "=========================== Sentence ======================\n",
      "Sentence: The Meidum pyramid was the first to have an above-ground burial chamber.\n",
      "Question: Was the Meidum pyramid the first to have an above-ground burial chamber?\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "parse_list = []\n",
    "count = 10 \n",
    "for sentence in sentences:\n",
    "    if len(sentence) < 500:\n",
    "        parse = next(parser.raw_parse(sentence))\n",
    "        if satisfies_structure(parse, simple_predicate) and binary_question_from_tree(parse):\n",
    "            count -= 1\n",
    "            \n",
    "            print(\"=========================== Sentence ======================\")\n",
    "            print(\"Sentence:\", sentence)\n",
    "            #print(parse)\n",
    "#             print(parse.label())\n",
    "            #print(sentence) \n",
    "            print(\"Question:\", binary_question_from_tree(parse))\n",
    "            parse_list.append(parse)\n",
    "            if count == 0:\n",
    "                break\n",
    "            \n",
    "\n",
    "print(count)  \n",
    "    \n",
    "#parse.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment_2",
   "language": "python",
   "name": "assignment_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

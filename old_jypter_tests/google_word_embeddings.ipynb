{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bit58b581aa823b48d68caa55b91f5030c4",
   "display_name": "Python 3.7.4 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use and setup pretrained Google Word Embeddings with Cosine Similarity\n",
    "\n",
    "Note: This uses a lot of RAM\n",
    "\n",
    "This provides the basic instructions for how to set it up, but I will also include it in the code below\n",
    "http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/\n",
    "\n",
    "The bin file needs to be downloaded before using this, so it should also be noted the file is ~3.64GB once unzipped\n",
    "\n",
    "Note: This also could be explored https://spacy.io/usage/vectors-similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://masongallo.github.io/machine/learning,/python/2016/07/29/cosine-similarity.html\n",
    "def cos_sim(a, b):\n",
    "\t\"\"\"Takes 2 vectors a, b and returns the cosine similarity according \n",
    "\tto the definition of the dot product\n",
    "\t\"\"\"\n",
    "\tdot_product = np.dot(a, b)\n",
    "\tnorm_a = np.linalg.norm(a)\n",
    "\tnorm_b = np.linalg.norm(b)\n",
    "\treturn dot_product / (norm_a * norm_b)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[ 3.93066406e-02 -1.96289062e-01 -1.06445312e-01 -2.68554688e-02\n  9.71679688e-02  9.91210938e-02 -5.88378906e-02 -2.75390625e-01\n  1.11816406e-01 -8.15429688e-02  2.53906250e-01 -3.49609375e-01\n -2.69531250e-01 -2.06298828e-02 -3.29589844e-02  2.39257812e-01\n  2.18505859e-02  1.29882812e-01 -5.95092773e-03  1.57226562e-01\n  1.34277344e-02 -9.08203125e-02  2.46093750e-01 -7.37304688e-02\n  7.27539062e-02 -1.43554688e-01 -2.02148438e-01  2.28515625e-01\n -1.42578125e-01 -7.47070312e-02  1.74804688e-01 -5.66406250e-02\n -1.77734375e-01  9.32617188e-02  6.21795654e-04  7.12890625e-02\n  2.53906250e-01  3.78417969e-02  3.17382812e-02 -8.74023438e-02\n  1.27929688e-01 -1.40625000e-01  2.05078125e-01 -6.64062500e-02\n -2.36511230e-03 -1.29882812e-01 -4.88281250e-02  5.03540039e-03\n -1.91650391e-02  6.59179688e-02 -1.05468750e-01 -4.91333008e-03\n  1.85546875e-01  7.17773438e-02 -1.48925781e-02 -5.02929688e-02\n -3.18359375e-01 -3.83300781e-02 -3.24707031e-02 -2.39257812e-02\n  2.55859375e-01  1.42578125e-01  1.10351562e-01  1.43432617e-02\n  1.07421875e-01  6.15234375e-02  3.71093750e-02 -3.41796875e-02\n  1.72851562e-01  4.17480469e-02  2.91015625e-01  8.98437500e-02\n -7.03125000e-02  2.57812500e-01 -4.23828125e-01  3.69140625e-01\n -5.24902344e-02 -3.44238281e-02 -7.86132812e-02  6.68945312e-02\n  1.35742188e-01  1.49414062e-01 -8.98437500e-02 -6.25000000e-02\n -7.47070312e-02  6.68945312e-02 -8.05664062e-02  4.07714844e-02\n -1.58691406e-02  6.46972656e-03  3.32031250e-02 -1.00097656e-01\n -9.42382812e-02 -1.75781250e-02 -1.05590820e-02 -6.03027344e-02\n  2.51953125e-01 -8.54492188e-02  2.85156250e-01  1.85546875e-02\n -3.37890625e-01 -3.35937500e-01  2.67578125e-01 -6.44531250e-02\n  8.78906250e-02 -7.08007812e-02  1.53320312e-01  1.58203125e-01\n  3.00781250e-01  1.34765625e-01  1.25000000e-01 -3.61328125e-02\n  2.77343750e-01 -6.68334961e-03 -1.22558594e-01 -3.45703125e-01\n -3.23486328e-03 -1.35742188e-01 -1.45874023e-02  2.02148438e-01\n  1.59179688e-01  1.18164062e-01 -2.86865234e-02 -7.56835938e-02\n -6.68945312e-02 -2.73437500e-01  1.62109375e-01 -8.05664062e-02\n  5.88378906e-02  1.58203125e-01 -1.14746094e-01 -7.61718750e-02\n -2.66113281e-02 -4.10156250e-02 -1.87988281e-02  5.00488281e-03\n  1.54296875e-01 -1.77734375e-01  1.82617188e-01 -3.19824219e-02\n  5.17578125e-02  9.17968750e-02  7.32421875e-02 -3.24707031e-02\n -7.44628906e-03  1.55273438e-01 -2.61718750e-01  1.39648438e-01\n -2.17285156e-02  1.35742188e-01  4.33593750e-01  1.90429688e-01\n  2.19726562e-01  1.82617188e-01 -7.56835938e-02 -1.67968750e-01\n  5.44433594e-02 -2.30468750e-01 -2.02636719e-02  3.44238281e-02\n -1.42578125e-01  1.51367188e-01  3.33984375e-01  2.81250000e-01\n -1.01318359e-02 -6.64062500e-02  2.33398438e-01  1.24511719e-02\n  1.09863281e-02  1.34765625e-01  1.43554688e-01 -3.24707031e-02\n  3.93066406e-02 -1.78710938e-01  6.68945312e-02  1.18652344e-01\n  2.85156250e-01  1.54296875e-01  1.33789062e-01  9.47265625e-02\n -1.72851562e-01  1.06201172e-02  1.19140625e-01 -1.02996826e-03\n  2.16796875e-01 -5.15747070e-03  1.52343750e-01  3.22265625e-01\n -1.23535156e-01 -1.01074219e-01 -1.29882812e-01  2.81250000e-01\n  1.26953125e-01  9.96093750e-02  1.39648438e-01  3.28125000e-01\n -4.54101562e-02  6.00585938e-02 -2.20947266e-02  2.01171875e-01\n -2.79296875e-01 -1.00097656e-01  2.03857422e-02  3.73535156e-02\n  3.65234375e-01 -2.20947266e-02 -7.65991211e-03  2.17773438e-01\n  1.12304688e-02 -3.80859375e-01  2.38037109e-02  6.10351562e-02\n -9.37500000e-02  2.81982422e-02  4.24804688e-02  9.86328125e-02\n -9.96093750e-02  3.26171875e-01 -2.02148438e-01 -7.56835938e-02\n -2.36816406e-02  1.82151794e-04  2.07031250e-01 -9.13085938e-02\n  1.59179688e-01 -3.73046875e-01  8.93554688e-02 -2.35595703e-02\n -1.03027344e-01 -2.06054688e-01 -6.22558594e-02  7.37304688e-02\n -1.58203125e-01 -5.10253906e-02 -2.11181641e-02 -1.68457031e-02\n -1.59179688e-01 -7.61718750e-02 -3.24218750e-01 -5.49316406e-02\n -2.17773438e-01 -1.04980469e-01 -9.47265625e-02 -6.39648438e-02\n  2.16064453e-02  3.10546875e-01  1.85546875e-01 -1.25000000e-01\n  1.31835938e-01 -3.53515625e-01  1.48437500e-01  1.53320312e-01\n  1.39648438e-01 -7.91015625e-02  3.75976562e-02 -9.42382812e-02\n -2.67578125e-01 -1.11328125e-01 -1.49414062e-01  5.88378906e-02\n  1.03027344e-01 -1.09863281e-01  9.76562500e-02 -2.50244141e-02\n  1.69921875e-01  2.98828125e-01 -1.60156250e-01 -7.76367188e-02\n  1.97753906e-02  4.46777344e-02  4.39453125e-03  1.23901367e-02\n -9.81445312e-02  1.38549805e-02  1.30859375e-01 -5.34667969e-02\n  1.41601562e-02  1.40625000e-01 -7.61718750e-02 -1.64062500e-01\n  2.61718750e-01 -2.83203125e-01 -2.90527344e-02 -5.41992188e-02\n  6.20117188e-02  2.18750000e-01 -1.14257812e-01 -3.22265625e-02\n -7.87353516e-03  4.06250000e-01 -2.17773438e-01  4.46777344e-02\n -2.42187500e-01 -3.75000000e-01 -1.25732422e-02 -2.84423828e-02\n -8.59375000e-02 -7.56835938e-02  4.66308594e-02 -8.39843750e-02]\n0.82941663\n"
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "word_vectors = model.wv #Note: This says it will be deprecated, but I could not get the recommended new method to work\n",
    "del model\n",
    "example = word_vectors[\"wife\"]\n",
    "print(example)\n",
    "\n",
    "# Cosine Similarity with these vectors\n",
    "X = word_vectors[\"wife\"]\n",
    "Y = word_vectors[\"husband\"]\n",
    "Z = word_vectors[\"woman\"]\n",
    "W = word_vectors[\"chair\"]\n",
    "\n",
    "X = X.reshape(-1, 1).flatten()\n",
    "Y = Y.reshape(-1, 1).flatten()\n",
    "singular_val = cos_sim(X, Y)\n",
    "\n",
    "print(singular_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Generation\n",
    "\n",
    "Purpose: Given an article output a list of sentences\n",
    "1. Parse Article into sentences\n",
    "2. From each sentence, generate Stanford dependency parse tree\n",
    "3. From each parse tree, use rule based method to generate question from sentence.\n",
    "4. Refine the sentences using language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article -> Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "for i in range(1, 2):\n",
    "    with open(f'./Development_data/set2/a{i}.txt', 'r') as f:\n",
    "        content.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for file in content:\n",
    "    sentences.extend(nltk.sent_tokenize(file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences -> Parse Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.corenlp import CoreNLPServer\n",
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "# nltk.download('wordnet')\n",
    "import os\n",
    "import requests\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "CoreNLPServerError",
     "evalue": "Could not connect to the server.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCoreNLPServerError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b82f7b1811e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m    \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTANFORD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stanford-corenlp-3.9.2-models.jar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/assignment_2/lib/python3.7/site-packages/nltk/parse/corenlp.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, stdout, stderr)\u001b[0m\n\u001b[1;32m    153\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCoreNLPServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Could not connect to the server.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCoreNLPServerError\u001b[0m: Could not connect to the server."
     ]
    }
   ],
   "source": [
    "STANFORD = os.path.join(\"models\", \"stanford-corenlp-full-2018-10-05\")\n",
    "\n",
    "# Create the server\n",
    "server = CoreNLPServer(\n",
    "   os.path.join(STANFORD, \"stanford-corenlp-3.9.2.jar\"),\n",
    "   os.path.join(STANFORD, \"stanford-corenlp-3.9.2-models.jar\"),    \n",
    ")\n",
    "server.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\n      \"tokens\": [\\n        {\\n          \"index\": 1,\\n          \"word\": \"data\",\\n          \"originalText\": \"data\",\\n          \"characterOffsetBegin\": 0,\\n          \"characterOffsetEnd\": 4,\\n          \"pos\": \"NN\",\\n          \"before\": \"\",\\n          \"after\": \"\"\\n        },\\n        {\\n          \"index\": 2,\\n          \"word\": \"=\",\\n          \"originalText\": \"=\",\\n          \"characterOffsetBegin\": 4,\\n          \"characterOffsetEnd\": 5,\\n          \"pos\": \"JJ\",\\n          \"before\": \"\",\\n          \"after\": \"\"\\n        },\\n        {\\n          \"index\": 3,\\n          \"word\": \"tmp\",\\n          \"originalText\": \"tmp\",\\n          \"characterOffsetBegin\": 5,\\n          \"characterOffsetEnd\": 8,\\n          \"pos\": \"NN\",\\n          \"before\": \"\",\\n          \"after\": \"\"\\n        }\\n      ]\\n    }\\n  ]\\n}\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.post('http://[::]:9000/?properties={\"annotators\":\"tokenize,ssplit,pos\",\"outputFormat\":\"json\"}', data = {'data': \"tmp\"}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Stanford Parser: https://nlp.stanford.edu/software/lex-parser.shtml#Download Version 3.9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tree import Tree\n",
    "parser = CoreNLPParser()\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "sp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "invertible_aux_verb = {'am', 'are', 'is', 'was', 'were', 'can', 'could', 'may', 'might',\n",
    "                       'must', 'shall', 'should', 'will', 'would'}\n",
    "invertible_special = {'does', 'did', 'has', 'had', 'have'}\n",
    "\n",
    "purge_tree = {\"PRN\", \"ADVP\", \"RB\"} #WHNP with parent SBAR #JJ, JJR, ADJP, S with parent NP\n",
    "\n",
    "np_requirements = {\"NNP\", \"NNPS\"}\n",
    "\n",
    "#Do/Did for I only\n",
    "#Does/Did for everything else\n",
    "\n",
    "def is_invertible(s, next_phrase):\n",
    "    if isinstance(s, str):\n",
    "        return (s.lower() in invertible_aux_verb or \n",
    "                s.lower() in invertible_special and next_phrase == \"VP\")\n",
    "    return False\n",
    "\n",
    "def list_to_string(word_list):\n",
    "    return ' '.join(word_list)\n",
    "\n",
    "def tree_to_string(parsed_tree, lower = False):\n",
    "#     if isinstance(parsed_tree, str):\n",
    "#         return parsed_tree\n",
    "#     words = []\n",
    "#     for subtree in parsed_tree:\n",
    "#         words.append(tree_to_string(subtree))\n",
    "    leaves = parsed_tree.leaves()\n",
    "    if lower:\n",
    "        leaves[0] = leaves[0].lower()\n",
    "    return list_to_string(leaves)\n",
    "\n",
    "def first(parsed_tree):\n",
    "    if isinstance(parsed_tree[0][0], str):\n",
    "        return parsed_tree[0], parsed_tree\n",
    "    return first(parsed_tree[0])\n",
    "\n",
    "def is_in(parsed_tree, label_set):\n",
    "    if isinstance(parsed_tree[0], str):\n",
    "        return parsed_tree.label() in label_set\n",
    "    contain = False\n",
    "    for tree in parsed_tree:\n",
    "        contain = contain or is_in(tree, label_set)\n",
    "    return contain\n",
    "\n",
    "#purges tree based on the set purge_trees\n",
    "def purge(parsed_tree):\n",
    "    if isinstance(parsed_tree, str):\n",
    "        return False\n",
    "    length = len(parsed_tree)\n",
    "    i = 0\n",
    "    if parsed_tree.label() in purge_tree:\n",
    "        return True\n",
    "    while i < length:\n",
    "        res = purge(parsed_tree[i])\n",
    "        if res:\n",
    "            del parsed_tree[i]\n",
    "            length -= 1\n",
    "        else:\n",
    "            i += 1\n",
    "    return False\n",
    "\n",
    "#keeps the first subtree that is of PP\n",
    "#pass in tree and a bool that checks for first\n",
    "\n",
    "#each recursive call we return tuple \n",
    "#(whether subtree needs to be deleted, what the new first is after running it on the tree)\n",
    "def purge_rest_helper(parsed_tree, first):\n",
    "    if isinstance(parsed_tree, str):\n",
    "        #is leaf\n",
    "        return (False, True)\n",
    "    if parsed_tree.label() == \"PP\":\n",
    "        #supposed to be purged\n",
    "        if first:\n",
    "            #delay purging\n",
    "            new_first = False\n",
    "        else:\n",
    "            #if its not the first, return first/False\n",
    "            return (True, False)\n",
    "    else:\n",
    "        new_first = first #true if first hasn't occured, false if it has\n",
    "        \n",
    "    length = len(parsed_tree)\n",
    "    #if its a first that is supposed to be purged you still look at the children\n",
    "    is_first = first #if this is in the first tree, we set the first in the loop to True else False\n",
    "    i = 0\n",
    "    while i < length:\n",
    "        (res, res_first) = purge_rest_helper(parsed_tree[i], is_first)\n",
    "        if res and not is_first:\n",
    "            del parsed_tree[i]\n",
    "            length -= 1\n",
    "        else:\n",
    "            i += 1\n",
    "        is_first = res_first\n",
    "    if not new_first:\n",
    "        return (False, new_first)\n",
    "    else:\n",
    "        return (False, is_first)\n",
    "\n",
    "def purge_except_first(parsed_tree):\n",
    "    purge_rest_helper(parsed_tree, True)\n",
    "\n",
    "# TODO: take care of situations where vp is inside a np\n",
    "# def binary_question_from_tree(parsed_tree):\n",
    "#     sentence = parsed_tree[0]\n",
    "#     assert(sentence.label() == 'S')\n",
    "#     np = sentence[0]\n",
    "#     vp = sentence[1]\n",
    "#     noun_label = first(np).label()\n",
    "#     #print(\"NL\", noun_label)\n",
    "#     assert(np.label() == 'NP')\n",
    "#     assert(vp.label() == 'VP')\n",
    "#     #print(parsed_tree)\n",
    "#     if is_in(np, np_requirements):\n",
    "#         subject = tree_to_string(np, True) if noun_label == \"DT\" else tree_to_string(np)\n",
    "#         #print(parsed_tree)\n",
    "#         #purge(vp)\n",
    "#         remain = vp.leaves()[1:]\n",
    "#         first_node, first_parent = first(vp)\n",
    "#         if is_invertible(first_node[0], first_parent[1].label()): #checks if is aux word\n",
    "#             return list_to_string([first(vp)[0].capitalize(), subject] + remain) + '?'\n",
    "#         else:\n",
    "#             #Add Does/Did/Do\n",
    "#             verb_label = first(vp).label()\n",
    "#             if isinstance(first(vp)[0], str):\n",
    "#                 lemmas = sp(first(vp)[0])\n",
    "#                 lemma = lemmas[0].lemma_\n",
    "#             if verb_label in [\"VBP\", \"VBZ\",\"VBG\"]: #present tense\n",
    "#                 return list_to_string([\"Does\", subject, lemma] + remain) + \"?\"\n",
    "#             elif verb_label in [\"VBD\", \"VBN\"]: #past tense\n",
    "#                 return list_to_string([\"Did\", subject, lemma] + remain) + \"?\"\n",
    "#     return None\n",
    "\n",
    "def binary_question_from_tree(parsed_tree):\n",
    "    sentence = parsed_tree[0]\n",
    "    assert(sentence.label() == 'S')\n",
    "    np = sentence[0]\n",
    "    vp = sentence[1]\n",
    "    if not isinstance(vp[0][0], str) or not isinstance(np[0][0], str):\n",
    "        return None\n",
    "    noun_label = np[0].label()\n",
    "    \n",
    "    assert(np.label() == 'NP')\n",
    "    assert(vp.label() == 'VP')\n",
    "   \n",
    "    if is_in(np, np_requirements) or :\n",
    "        subject = tree_to_string(np, True) if noun_label == \"DT\" else tree_to_string(np)\n",
    "        remain = vp.leaves()[1:]\n",
    "        if is_invertible(vp[0][0],vp[1].label()): #checks if is aux word\n",
    "            return list_to_string([vp[0][0].capitalize(), subject] + remain) + '?'\n",
    "        else:\n",
    "            #Add Does/Did/Do\n",
    "            verb_label = vp[0].label()\n",
    "            lemmas = sp(vp[0][0])\n",
    "            lemma = lemmas[0].lemma_\n",
    "            if verb_label in [\"VBP\", \"VBZ\",\"VBG\"]: #present tense\n",
    "                return list_to_string([\"Does\", subject, lemma] + remain) + \"?\"\n",
    "            elif verb_label in [\"VBD\", \"VBN\"]: #past tense\n",
    "                return list_to_string([\"Did\", subject, lemma] + remain) + \"?\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence Structure Tree\n",
    "class SST():\n",
    "    def __init__(self, label, children):\n",
    "        self.label = label\n",
    "        self.children = children\n",
    "\n",
    "#Sentence Structure Leaf\n",
    "class SSL():\n",
    "    def __init__(self, label):\n",
    "        self.label = label\n",
    "        \n",
    "simple_predicate = SST('ROOT', [SST('S', [SSL('NP'), SSL('VP'), SSL('.')])])\n",
    "\n",
    "def satisfies_structure(parsed_tree, structure):\n",
    "    if isinstance(structure, SSL):\n",
    "        return parsed_tree.label() == structure.label\n",
    "    else:\n",
    "        if parsed_tree.label() != structure.label or len(parsed_tree) != len(structure.children): return False\n",
    "        for i in range(len(parsed_tree)):\n",
    "            if satisfies_structure(parsed_tree[i], structure.children[i]) == False:\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "# sentences = [\"Joe has an apple\", \"Joe has done something wrong\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the Indus cities noted for their urban planning , baked brick houses , elaborate drainage systems , water supply systems , and clusters of large non-residential buildings?\n",
      "Is the Indus Valley Civilisation named the Harappan civilisation after Harappa , the first of its sites to be excavated in the 1920s , in what was the Punjab province of British India?\n",
      "Is the Indus Valley Civilisation named the Harappan civilisation after Harappa , the first of its sites to be excavated in the 1920s , in what was the Punjab province of British India?\n",
      "Has the Indus Valley Civilisation been called by some the `` Sarasvati culture '' , the `` Sarasvati Civilisation '' , the `` Indus-Sarasvati Civilisation '' or the `` Sindhu-Saraswati Civilisation '' , as the Ghaggar-Hakra river is identified by some with the mythological Sarasvati river , suggesting that the Indus Valley Civilisation was the Vedic civilisation as perceived by traditional Hindu beliefs?\n",
      "Has an Indus Valley site been found on the Oxus River at Shortughai in northern Afghanistan , in the Gomal River valley in northwestern Pakistan , at Manda , Jammu on the Beas River near Jammu , India , and at Alamgirpur on the Hindon River , 28 km from Delhi?\n",
      "Have Indus Valley sites been found on rivers , but on the ancient seacoast , for example , Balakot , and on islands , for example , Dholavira?\n",
      "Did John write , `` I was exercised in my mind how we were to get ballast for the line of the railway ''?\n",
      "Was the Indus Valley Civilisation site hit by 10 feet of water as the Sutlej Yamuna link canal overflowed?\n",
      "Is Hakra Ware culture a material culture which is contemporaneous with the early Harappan Ravi phase culture of the Indus Valley?\n",
      "Is Mehrgarh one of the earliest sites with evidence of farming and herding in South Asia?\n",
      "Does Lukacs and Hemphill suggest an initial local development of Mehrgarh , with a continuity in cultural development but a change in population?\n",
      "Does Kot Diji represent the phase leading up to Mature Harappan , with the citadel representing centralised authority and an urban quality of life?\n",
      "Did Trade networks link this culture with related regional cultures and distant sources of raw materials , including lapis lazuli and other materials for bead-making?\n",
      "Did Sir John Marshall identify a resemblance to the Hindu god , Shiva?\n",
      "Did the Harappans make various toys and games , among them cubical dice , which were found in sites like Mohenjo-Daro?\n",
      "Does Archaeologist Jim G. Shaffer write that the Mehrgarh site `` demonstrates that food production was an indigenous South Asian phenomenon '' and that the data support interpretation of `` the prehistoric urbanisation and complex social organisation in South Asia based on indigenous , but isolated , cultural developments ''?\n",
      "Is Zebu cattle common in India , and in Africa?\n",
      "Does Finnish Indologist Asko Parpola conclude that the uniformity of the Indus inscriptions precludes any possibility of different languages being used , and that an early form of Dravidian language must have been the language of the Indus people?\n",
      "Have Farmer , Sproat , and Witzel disputed this finding , pointing out that Rao et al.?\n",
      "Does One Indus Valley seal show a seated figure with a horned headdress , tricephalic and ithyphallic , surrounded by animals?\n",
      "Did Marshall identify the figure as an early form of the Hindu god Shiva , who is associated with asceticism , yoga , and linga ; regarded as a lord of animals ; and depicted as having three eyes?\n",
      "Has Doris Srinivasan argued that the figure does have three faces , or yogic posture , and that in Vedic literature Rudra was a protector of wild animals?\n",
      "Did Herbert Sullivan and Alf Hiltebeitel reject Marshall 's conclusions , with the former claiming that the figure was female , while the latter associated the figure with Mahisha , the Buffalo God and the surrounding animals with vahanas of deities for the four cardinal directions?\n",
      "Does Many Indus Valley seals show animals , with some depicting them being carried in processions , while others show chimeric creations?\n",
      "Did the Indus Valley climate grow cooler and drier from about 1800 BCE , linked to a general weakening of the monsoon at that time?\n",
      "Does David Gordon White cites three other mainstream scholars who `` have demonstrated '' that Vedic religion derives from the Indus Valley Civilisations?\n",
      "Does Harvard archaeologist Richard Meadow point to the late Harappan settlement of Pirak , which thrived from 1800 BCE to the time of the invasion of Alexander the Great in 325 BCE?\n",
      "Has the IVC been compared in particular with the civilisations of Elam and with Minoan Crete?\n",
      "Did Mortimer Wheeler interpret the presence of many unburied corpses found in the top levels of Mohenjo-daro as the victims of a warlike conquest , and stated that `` Indra stands accused '' of the destruction of the IVC?\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "parse_list = []\n",
    "count = 0\n",
    "for sentence in sentences:\n",
    "    if len(sentence) < 500:\n",
    "        parse = next(parser.raw_parse(sentence))\n",
    "        #print(parse)\n",
    "        purge(parse)\n",
    "        tmp_parse = copy.deepcopy(parse)\n",
    "        #purge_except_first(parse)\n",
    "        if satisfies_structure(parse, simple_predicate):\n",
    "            question = binary_question_from_tree(parse)\n",
    "            if question != None:\n",
    "                count += 1\n",
    "                #print(\"=========================== Sentence ======================\")\n",
    "                #print(\"Sentence:\", sentence)\n",
    "                #print(tmp_parse)\n",
    "    #             print(parse.label())\n",
    "                #print(sentence) \n",
    "                print(question)\n",
    "                parse_list.append(parse)\n",
    "                if count == 0:\n",
    "                    break\n",
    "\n",
    "\n",
    "print(count)  \n",
    "    \n",
    "#parse.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Angela ', 'lives ', 'in ', 'Boston ', 'as ', 'she ', 'is ', 'working ', 'there', '. ', 'She ', 'is ', 'quite ', 'happy ', 'in ', 'that ', 'city', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tree import Tree\n",
    "import neuralcoref\n",
    "\n",
    "sp = spacy.load('en_core_web_lg')\n",
    "neuralcoref.add_to_pipe(sp)\n",
    "doc2 = sp('Angela lives in Boston as she is working there. She is quite happy in that city.')\n",
    "print(list(tok.text_with_ws for tok in doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Angela: [Angela, she, She], Boston: [Boston, that city]]\n",
      "Angela lives in Boston as she is working there.\n",
      "She is quite happy in that city.\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "set() Angela Angela: [Angela, she, She]\n",
      "{0} she Angela: [Angela, she, She]\n",
      "{0} She Angela: [Angela, she, She]\n",
      "set() Boston Boston: [Boston, that city]\n",
      "{0} that city Boston: [Boston, that city]\n",
      "Angela lives in Boston as she is working there. Angela is quite happy in Boston.\n"
     ]
    }
   ],
   "source": [
    "def get_resolved(doc, clusters):\n",
    "    ''' Return a list of utterrances text where the coref are resolved to the most representative mention'''\n",
    "    sentences = [sent.string.strip() for sent in doc.sents]\n",
    "    \n",
    "    token_labels = []\n",
    "    for i in range(len(sentences)):\n",
    "        print(sentences[i])\n",
    "        for j in range(len(list(tok.text_with_ws for tok in sp(sentences[i])))):\n",
    "            token_labels.append(i)\n",
    "    print(token_labels)\n",
    "    resolved = list(tok.text_with_ws for tok in doc)\n",
    "    for cluster in clusters:\n",
    "        seen = set()\n",
    "        for coref in cluster:\n",
    "            print(seen, coref, cluster)\n",
    "            if coref != cluster.main and token_labels[coref.start] not in seen:\n",
    "                resolved[coref.start] = cluster.main.text + doc[coref.end-1].whitespace_\n",
    "                for i in range(coref.start+1, coref.end):\n",
    "                    resolved[i] = \"\"\n",
    "            seen.add(token_labels[coref.start])\n",
    "    return ''.join(resolved)\n",
    "print(doc2._.coref_clusters)\n",
    "print(get_resolved(doc2, doc2._.coref_clusters))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'server' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-934092d45e39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'server' is not defined"
     ]
    }
   ],
   "source": [
    "server.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment_2",
   "language": "python",
   "name": "assignment_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
